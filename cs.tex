\documentclass[8pt]{article}
% Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Page layout
\geometry{a4paper, margin=0.5in}

\begin{document}

\begin{multicols}{2}
\section*{Exam Cheat Sheet}

% Topic 1: Discriminative Functions for Classifiers
\subsection*{Discriminative Functions for Classifiers}
\textbf{Naive Bayes}
\begin{equation}
    P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\end{equation}

\begin{equation}
    P(x|y) = \prod_{i=1}^{n} P(x_i|y)
\end{equation}

\textbf{Fisher's Linear Discriminant Analysis}
\begin{equation}
    w = S_W^{-1} (\mu_1 - \mu_2)
\end{equation}

\begin{multline}
    S_W = \frac{1}{N_1} \sum_{n \in C_1} (x_n - \mu_1)(x_n - \mu_1)^T \\
    + \frac{1}{N_2} \sum_{n \in C_2} (x_n - \mu_2)(x_n - \mu_2)^T
\end{multline}

\begin{equation}
    S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
\end{equation}

\textbf{Logistic Regression}
\begin{equation}
    \phi(x) = \frac{1}{1 + e^{-x}}
\end{equation}

\begin{equation}
    y(x) = \phi(w^T x)
\end{equation}

\begin{equation}
    J(\theta) = -\frac{1}{m} [y^T\log(\sigma(X\theta)) + (1-y)^T\log(1-\sigma(X\theta))]
\end{equation}

\textbf{Perceptron Algorithm}
\begin{equation}
    \mathcal{E}_p (\mathbf{w}, w_0) = -\sum_{n \in M} t_n(\mathbf{w} \cdot \mathbf{x}_n + w_0), \text{ where } M \text{ is misclass} 
\end{equation}

\textbf{Gradient Descent}
\begin{equation}
    \theta := \theta - \alpha \nabla_{\theta} J(\theta)
\end{equation}

\subsection*{Support Vector Machines}
% Topic 2: Support Vector Machines
% Kernel Machines
\textbf{Kernel Machines}
% RBF or Radial Basis Function Kernel
\begin{equation}
    K(x, x') = \exp(-\frac{\|x - x'\|^2}{2\sigma^2}) = \exp(-\gamma \|x - x'\|^2)
\end{equation}

\begin{equation}
    \gamma = \frac{1}{2\sigma^2}
\end{equation}

\textbf{Hard Margin SVM}
\begin{equation}
    \min_{w, b} \frac{1}{2} \|w\|^2 \text{ s.t. } y_i(w^Tx_i + b) \geq 1
\end{equation}

\begin{equation}
    \mathcal{L}(\mathbf{w}, w_0, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^{N} \alpha_i[t_i(\mathbf{w}^T\mathbf{x}_i + w_0) - 1]
\end{equation}

\textbf{Soft Margin SVM}
\begin{multline}
    \mathcal{L}(\mathbf{w}, w_0, \mathbf{\alpha}, \mathbf{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i - \\
    \sum_{i=1}^{N} \alpha_i[t_i(\mathbf{w} \cdot \mathbf{x}_i + w_0) - 1 + \xi_i] - \sum_{i=1}^{N} \mu_i \xi_i
\end{multline}

\subsection*{Performance Metrics}
\textbf{Confusion Matrix}
\begin{equation}
    \begin{bmatrix}
        TP & FP \\
        FN & TN
    \end{bmatrix}
\end{equation}

\textbf{Precision}
\begin{equation}
    \frac{TP}{TP + FP}
\end{equation}

\textbf{Recall}
\begin{equation}
    \frac{TP}{TP + FN}
\end{equation}

\textbf{F1 Score}
\begin{equation}
    \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\textbf{Accuracy}
\begin{equation}
    \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

\textbf{ROC Curve}
\begin{equation}
    \text{TPR} = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
    \text{FPR} = \frac{FP}{FP + TN}
\end{equation}

\subsection*{Dimensionality Reduction}
\textbf{PCA}
\begin{equation}
    \mathbf{X} = \mathbf{X} - \bar{\mathbf{X}}
\end{equation}

% Finding Principal Components used mean centered data
\begin{equation}
    \mathbf{Cov} = \frac{1}{N - 1} \mathbf{X}^T \mathbf{X}
\end{equation}

% Finding the Eigenvectors and Eigenvalues of the Covariance Matrix
\begin{equation}
    \mathbf{C_{ov}} \mathbf{v} = \lambda \mathbf{v}
\end{equation}

\begin{equation}
    C_{ov} = \mathbf{E}[\mathbf{XX^T}]
\end{equation}
When $\mathbf{X}$ is mean-centered.

\end{multicols}
\end{document}
