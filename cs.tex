\documentclass[10pt]{article}
% Packages
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{amssymb}

% Page layout
\geometry{a4paper, margin=0.5in}

\begin{document}

\begin{multicols}{2}
\section*{Exam Cheat Sheet}

% Topic 1: Discriminative Functions for Classifiers
\subsection*{Discriminative Functions for Classifiers}
\textbf{Naive Bayes}
\begin{equation}
    P(y|x) = \frac{P(x|y)P(y)}{P(x)}
\end{equation}

\begin{equation}
    P(x|y) = \prod_{i=1}^{n} P(x_i|y)
\end{equation}

\textbf{Fisher's Linear Discriminant Analysis}
\begin{equation}
    w = S_W^{-1} (\mu_1 - \mu_2)
\end{equation}

\begin{multline}
    S_W = \frac{1}{N_1} \sum_{n \in C_1} (x_n - \mu_1)(x_n - \mu_1)^T \\
    + \frac{1}{N_2} \sum_{n \in C_2} (x_n - \mu_2)(x_n - \mu_2)^T
\end{multline}

\begin{equation}
    S_B = (\mu_1 - \mu_2)(\mu_1 - \mu_2)^T
\end{equation}

\textbf{Logistic Regression}
\begin{equation}
    \phi(x) = \frac{1}{1 + e^{-x}}
\end{equation}

\begin{equation}
    y(x) = \phi(w^T x)
\end{equation}

\begin{equation}
    J(\theta) = -\frac{1}{m} [y^T\log(\sigma(X\theta)) + (1-y)^T\log(1-\sigma(X\theta))]
\end{equation}

\textbf{Perceptron Algorithm}
\begin{equation}
    \mathcal{E}_p (\mathbf{w}, w_0) = -\sum_{n \in M} t_n(\mathbf{w} \cdot \mathbf{x}_n + w_0), \text{ where } M \text{ is misclass} 
\end{equation}

\textbf{MLP}
\begin{multline}
    \begin{bmatrix}
        y_1 \\
        y_2 \\
        \vdots \\
        y_m
    \end{bmatrix} = \phi(\mathbf{W}^T \mathbf{X} + \mathbf{b}) \\
    = \phi(\mathbf{W}^T \phi(\mathbf{V}^T \mathbf{X} + \mathbf{c}) + \mathbf{b})
\end{multline}

\textbf{Gradient Descent}
\begin{equation}
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla J(\mathbf{w}^{(t)})
\end{equation}

\subsection*{Regularization \& Optimization}

\subsubsection*{Pooling Techniques}

\textbf{Max Pooling}
\begin{equation}
    \text{Max Pooling: } y_{i,j} = \max_{m,n} x_{i+m, j+n}
\end{equation}

\textbf{Average Pooling}
\begin{equation}
    \text{Average Pooling: } y_{i,j} = \frac{1}{mn} \sum_{m,n} x_{i+m, j+n}
\end{equation}

\textbf{Batch Normalization}
\begin{equation}
    \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}
\end{equation}
Where $\mu$ is the mean, $\sigma^2$ is the variance, and $\epsilon$ is a small constant to prevent division by zero.

\subsubsection*{Gradient Descent Methods}

\textbf{Stochastic Gradient Descent (SGD)}
\begin{equation}
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla J(\mathbf{w}^{(t)}, \mathbf{x}_n, t_n)
\end{equation}
Where $\mathbf{x}_n$ is a randomly selected training example, $\eta$ is the learning rate.

\textbf{Nesterov's Accelerated Gradient (NAG)}
\begin{equation}
    \mathbf{v}^{(t+1)} = \gamma \mathbf{v}^{(t)} + \eta \nabla J(\mathbf{w}^{(t)} - \gamma \mathbf{v}^{(t)})
\end{equation}
Where $\gamma$ is the momentum term.

\subsubsection*{Adaptive Moment Estimation Methods}

\textbf{Adam}
\begin{equation}
    m^{(t+1)} = \beta_1 m^{(t)} + (1 - \beta_1) \nabla J(\mathbf{w}^{(t)})
\end{equation}

\begin{equation}
    v^{(t+1)} = \beta_2 v^{(t)} + (1 - \beta_2) \nabla J(\mathbf{w}^{(t)})^2
\end{equation}

\begin{equation}
    \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \frac{m^{(t+1)}}{\sqrt{v^{(t+1)} + \epsilon}}
\end{equation}

Where $m$ and $v$ are the first and second moment estimates, respectively, $\beta_1$ and $\beta_2$ are the decay rates, and $\epsilon$ is a small constant to prevent division by zero.

\textbf{Nadam}
\begin{equation}
    m^{(t+1)} = \beta_1 m^{(t)} + (1 - \beta_1) \nabla J(\mathbf{w}^{(t)})
\end{equation}
\begin{equation}
    v^{(t+1)} = \beta_2 v^{(t)} + (1 - \beta_2) \nabla J(\mathbf{w}^{(t)})^2
\end{equation}
Where $m$ and $v$ are the first and second moment estimates, respectively, $\beta_1$ and $\beta_2$ are the decay rates, and $\epsilon$ is a small constant to prevent division by zero.


\subsection*{Support Vector Machines}
% Topic 2: Support Vector Machines
% Kernel Machines
\textbf{Kernel Machines}
% RBF or Radial Basis Function Kernel
\begin{equation}
    K(x, x') = \exp(-\frac{\|x - x'\|^2}{2\sigma^2}) = \exp(-\gamma \|x - x'\|^2)
\end{equation}

\begin{equation}
    \gamma = \frac{1}{2\sigma^2}
\end{equation}

\textbf{Hard Margin SVM}
\begin{equation}
    \min_{w, b} \frac{1}{2} \|w\|^2 \text{ s.t. } y_i(w^Tx_i + b) \geq 1
\end{equation}

\begin{equation}
    \mathcal{L}(\mathbf{w}, w_0, \alpha) = \frac{1}{2} \|\mathbf{w}\|^2 - \sum_{i=1}^{N} \alpha_i[t_i(\mathbf{w}^T\mathbf{x}_i + w_0) - 1]
\end{equation}

\textbf{Soft Margin SVM}
\begin{multline}
    \mathcal{L}(\mathbf{w}, w_0, \mathbf{\alpha}, \mathbf{\mu}) = \frac{1}{2} \|\mathbf{w}\|^2 + C \sum_{i=1}^{N} \xi_i - \\
    \sum_{i=1}^{N} \alpha_i[t_i(\mathbf{w} \cdot \mathbf{x}_i + w_0) - 1 + \xi_i] - \sum_{i=1}^{N} \mu_i \xi_i
\end{multline}

\subsection*{Performance Metrics}
\textbf{Confusion Matrix}
\begin{equation}
    \begin{bmatrix}
        TP & FP \\
        FN & TN
    \end{bmatrix}
\end{equation}

\textbf{Precision}
\begin{equation}
    \frac{TP}{TP + FP}
\end{equation}

\textbf{Recall}
\begin{equation}
    \frac{TP}{TP + FN}
\end{equation}

\textbf{F1 Score}
\begin{equation}
    \frac{2 \times \text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
\end{equation}

\textbf{Accuracy}
\begin{equation}
    \frac{TP + TN}{TP + FP + FN + TN}
\end{equation}

\textbf{ROC Curve}
\begin{equation}
    \text{TPR} = \frac{TP}{TP + FN}
\end{equation}

\begin{equation}
    \text{FPR} = \frac{FP}{FP + TN}
\end{equation}

\subsection*{Dimensionality Reduction}
\textbf{PCA}
\begin{equation}
    \mathbf{X} = \mathbf{X} - \bar{\mathbf{X}}
\end{equation}

% Finding Principal Components used mean centered data
\begin{equation}
    \mathbf{Cov} = \frac{1}{N} \mathbf{X}^T \mathbf{X}
\end{equation}

\subsubsection*{Eigenvectors and Eigenvalues}
Eigendeomposition of the $3 \times 3$ covariance matrix.
\begin{equation}
    \mathbf{Cov} = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^T
\end{equation}
Where $\mathbf{Q}$ is the matrix of eigenvectors and $\mathbf{\Lambda}$ is the diagonal matrix of eigenvalues.

% Finding the Eigenvectors and Eigenvalues of the Covariance Matrix
\begin{equation}
    \mathbf{C_{ov}} \mathbf{v} = \lambda \mathbf{v}
\end{equation}

\begin{equation}
    C_{ov} = \mathbf{E}[\mathbf{XX^T}]
\end{equation}
When $\mathbf{X}$ is mean-centered.

\subsubsection*{Manifold Learning}
\begin{equation}
    \mathbf{d}_{Euclidean} = \sqrt{\sum_{i=1}^{N} (x_i - x_j)^2}
\end{equation}

\begin{equation}
    \mathbf{d}_{Geodesic} = \min_{\mathbf{p}} \sum_{i=1}^{N-1} \sqrt{\sum_{j=1}^{N} (p_{i,j} - p_{i+1,j})^2}
    \label{eq:geodesic}
\end{equation}

\begin{equation}
    \mathbf{D} = \begin{bmatrix}
        d_{1,1} & d_{1,2} & \cdots & d_{1,N} \\
        d_{2,1} & d_{2,2} & \cdots & d_{2,N} \\
        \vdots & \vdots & \ddots & \vdots \\
        d_{N,1} & d_{N,2} & \cdots & d_{N,N} \\
    \end{bmatrix}
\end{equation}

\begin{equation}
    \mathbf{D}^2 = \begin{bmatrix}
        d_{1,1}^2 & d_{1,2}^2 & \cdots & d_{1,N}^2 \\
        d_{2,1}^2 & d_{2,2}^2 & \cdots & d_{2,N}^2 \\
        \vdots & \vdots & \ddots & \vdots \\
        d_{N,1}^2 & d_{N,2}^2 & \cdots & d_{N,N}^2 \\
    \end{bmatrix}
\end{equation}

\begin{equation}
    d_{i,j}^2 = b_{i,i} + b_{j,j} - 2b_{i,j}
\end{equation}
Where $b_{i,j}$ is the element of the matrix $\mathbf{B}$.

\begin{equation}
    \mathbf{B} = -\frac{1}{2} \mathbf{J} \mathbf{D}^2 \mathbf{J}
\end{equation}

\begin{equation}
    \mathbf{J} = \mathbf{I} - \frac{1}{N} \mathbf{1} \mathbf{1}^T
\end{equation}
Where $\mathbf{I}$ is the identity matrix and $\mathbf{1}$ is a vector of ones and $\mathbf{11}^T$ is the outer product of $\mathbf{1}$.


\end{multicols}
\end{document}
